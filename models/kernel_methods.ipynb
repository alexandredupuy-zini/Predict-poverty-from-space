{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BglU2CgMyxwt"
   },
   "source": [
    "# Kernel methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AgY45NVlzA-w"
   },
   "source": [
    "The regularized empirical risk minimization problem is : $\\hat f = argmin_{f \\in F} \\frac{1}{n} \\sum_{i = 1}^n L(y_i, f(x_i)) + \\lambda \\Omega (f)$\n",
    "\n",
    "A simple example, linear models : $X = R^p$, $F = \\{ f_w : x \\mapsto w^T x | w \\in R^p \\}$, $\\Omega (f_w) = ||w||^2_2$\n",
    "\n",
    "By choosing carefully the loss function, we can create several well-known models :\n",
    "- ridge regression : $L(y_i, w^T x_i) = \\frac{1}{2} (y_i - w^T x_i)^2$\n",
    "- linear SVM : $L(y_i, w^T x_i) = max(0, 1 - y_i w^T x_i)$\n",
    "- logistic regression : $L(y_i, w^T x_i) = log(1 + e^{-y_i w^T x_i})$\n",
    "\n",
    "Unfortunately, linear models often perform poorly unless the problem features are well-engineered or the problem is very simple.\n",
    "\n",
    "To solve this problem, we need to change the functional space F :  \n",
    "1. By choosing F as a deep learning space  \n",
    "2. By choosing F as a RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G3jC9FPRy0EF"
   },
   "source": [
    "## I) Kernels and RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcJQ0J28y4qQ"
   },
   "source": [
    "### 1) Positive definite kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d5xQyDMGzADq"
   },
   "source": [
    "The kernel method is based on pairwise comparisons between data points. We define a \"comparison function\" $K : X^2 \\to R$ and represent a set of n data points $S = { x_1, ... x_n}$ by the n x n matrix $K = [K(x_i, x_j)]_{1 <= i,j <= n}$. However, we will restrict ourselves to a particular class of pairwise comparison functions : positive definite kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wkEvpopM1vaX"
   },
   "source": [
    "**Definition :** A **positive definite kernel** on the set X is a function $K : X^2 \\to R$ that is symmetric and which satisfies : $$\\forall n \\in N, \\forall x_1, ... x_n \\in X^n, \\forall a_1, ... a_n \\in R^n, \\sum_i \\sum_j a_i a_j K(x_i, x_j) \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CPNBetLv2d6k"
   },
   "source": [
    "Equivalently, a kernel K is pd if and only if for any set of n data points, the associated matrix K is symmetric and positive semidefinite.\n",
    "\n",
    "$\\forall n \\in N, \\forall x_1, ... x_n \\in X^n, K = [K(x_i, x_j)]_{1 <= i,j <= n}$ is symmetric and positive semidefinite, ie :\n",
    "$$K^T = K$$\n",
    "$$\\forall u \\in R^n, u^T K u \\geq 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yDWwQi-52seF"
   },
   "source": [
    "> __Example : linear kernel__  \n",
    "$X = R^d$\n",
    "$$K : X^2 \\to R \\\\\n",
    "(x, y) \\mapsto \\langle x, y \\rangle$$\n",
    "K is symmetric by definition of the inner product in $R^d$ and verifies : $\\sum_i \\sum_j a_i a_j K(x_i, x_j) = \\sum_i \\sum_j a_i a_j \\langle x_i, x_j \\rangle = \\langle \\sum_i a_i x_i, \\sum_j a_j x_j \\rangle = || \\sum_i a_i x_i ||^2 \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9rhaSC-y5YFU"
   },
   "source": [
    "**Lemma :** $\\phi : X \\to R^d$. If \n",
    "$$K : X^2 \\to R \\\\\n",
    "(x, y) \\mapsto \\langle \\phi(x), \\phi(y) \\rangle$$\n",
    "Then K is a pd kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7s6ftFP56g3"
   },
   "source": [
    "> **Proof :** K is symmetric by definition of the inner product in $R^d$ and verifies : $\\sum_i \\sum_j a_i a_j K(x_i, x_j) = \\sum_i \\sum_j a_i a_j \\langle \\phi(x_i), \\phi(x_j) \\rangle = \\langle \\sum_i a_i \\phi(x_i), \\sum_j a_j \\phi(x_j) \\rangle = || \\sum_i a_i \\phi(x_i) ||^2 \\geq 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8--Ibkcs6n4n"
   },
   "source": [
    "> __Example : polynomial kernel__  \n",
    "$X = R^2$\n",
    "$$K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$$\n",
    "where $\\phi : R^2 \\to R^3 \\\\ x = (x_1, x_2) \\mapsto (x_1^2, \\sqrt2x_1x_2, x_2^2)$  \n",
    "Then K is a pd kernel and we can show that :  \n",
    "$$K(x, y) = x_1^2 y_1^2 + 2x_1x_2y_1y_2 + x_2^2y_2^2 = (x_1y_1 + x_2y_2)^2 = \\langle x, y \\rangle ^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHVnrsgE79Z-"
   },
   "source": [
    "The converse of the previous lemma is a fundamental theorem in kernel methods : it shows that any pd kernels can be considered as an inner product in a Hilbert space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Fu00Dkw79cu"
   },
   "source": [
    "**Theorem : K is a pd kernel if and only if there exists a Hilbert space H and a mapping $\\phi : X \\to H$ such that \n",
    "$$\\forall x, y \\in H^2, K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9uwvkhB9Bzj"
   },
   "source": [
    "> **Proof :** finite case  \n",
    "($\\Leftarrow$) Already proved.  \n",
    "($\\Rightarrow$) Assume $X = {x_1, ... x_N}$ is finite of size N.  \n",
    "Any positive definite kernel $K : X \\times X \\to R$ is entirely defined by the $N \\times N$ symmetric positive semidefinite matrix $K = [(K(x_i, x_j)]_{i, j}$.  \n",
    "It can therefore be diagonalized (why?) on an orthonormal basis of eigenvectors $(u_1, ... u_N)$, with non-negative eigenvalues $0 \\leq \\lambda_1 \\leq ... \\leq \\lambda_N$, i.e.,\n",
    "$$K_{i, j} = K(x_i, x_j) = [\\sum_{k=1}^N \\lambda_k u_k u_k^T]_{i,j} = \\sum_{k=1}^N \\lambda_k u_{ik} u_{jk} = \\langle \\phi(x_i), \\phi(x_j) \\rangle$$\n",
    "with\n",
    "$$\\phi(x_i) = (\\sqrt{\\lambda_1} u_{i1}, ... \\sqrt{\\lambda_N}u_{iN})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ub_0WD399ESz"
   },
   "source": [
    "> **Proofs :**\n",
    "- if X is a compact and K continuous : Mercer's proof\n",
    "- if X is countable : Kolmogorov's proof\n",
    "- for the general case : Aronszajn's proof"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EM65WrM49bCN"
   },
   "source": [
    "We will go through the proof of the general case by introducing the concept of Reproducing Kernel Hilbert Space (RKHS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nTTT0g9Z9fcE"
   },
   "source": [
    "### 2) Reproducing Kernel Hilbert Space (RKHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6M32Wi459mPm"
   },
   "source": [
    "**Definition :**  Let X be a set, $H \\subset R^X$ a class of functions $X \\to R$ forming a (real) Hilbert space (with inner product $\\langle ., . \\rangle$).  \n",
    "The function $K : X^2 \\to R$ is called a **reproducing kernel** of the Hilbert space H if and only if :\n",
    "- H contains all functions of the form : $\\forall x \\in X, K_x : t \\to K(x, t)$\n",
    "- For all $x \\in X$ and for all $f \\in H$, the reproducing property holds : $f(x) = \\langle f, K_x \\rangle$\n",
    "\n",
    "If a reproducing kernel of H exists, then H is called a **reproducing kernel Hilbert space** (RKHS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFcO1etNoVl2"
   },
   "source": [
    "**Theorem : The Hilbert space H is a RKHS if and only if for all $x \\in X$ $$F : H \\to R \\\\ f \\mapsto f(x)$$ is continuous.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcyplKNbpEH3"
   },
   "source": [
    "> **Proof :**  \n",
    "($\\Rightarrow$) H is a RKHS. We wonder if $$F : H \\to R \\\\ f \\mapsto f(x)$$ is continuous  \n",
    "We can show that F is L-smooth. Because H is a RKHS, there exists a reproducing kernel K and for any $x \\in X$ and any $f, g \\in H^2$ :\n",
    "$$ || F(f) - F(g) || = | f(x) - g(x) | = \n",
    "| \\langle f - g, K_x \\rangle | \\\\\\leq || f - g ||_H . || K_x ||_H \\leq || f - g ||_H . \\sqrt \\langle K_x, K_x \\rangle \\\\\\leq || f - g ||_H . \\sqrt{K(x, x)}$$\n",
    "Hence, F is L-smooth (with $L = \\sqrt{K(x, x)}$) and thus continuous.  \n",
    "($\\Leftarrow$) F is continuous. We want to show that H is a RKHS, i.e. there exists a reproducing kernel K for H  \n",
    "By using the **Riesz representation theorem** (an important property of Hilbert spaces) : if H is an Hilbert space then any continuous linear form f on H can be written as the inner product such that $f(.) = \\langle ., y \\rangle$ where $y \\in H$ is unique  \n",
    "Yet, F is a continuous linear form on H where the elements of H are functions. Hence :\n",
    "$$ \\forall x \\in X, \\exists ! g_x \\in H, F(f) = f(x) = \\langle f, g_x \\rangle$$\n",
    "Finally, the function $K(x, y) = g_x (y)$ is a rk for H because it holds the reproducing property and $\\forall x \\in X, g_x \\in H$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipr7WBjzxtJO"
   },
   "source": [
    "**Corollary :** Convergence in a RKHS implies pointwise convergence, i.e. if $(f_n)_{n \\in N}$ converges to $f$ in H, then, for any $x \\in X$, $(f_n(x))_{n \\in N}$ converges to $f(x)$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem :  \n",
    "\\- If H is a RKHS, then it has a unique reproducing kernel  \n",
    "\\- Conversely, a function K can be the reproducing kernel of at most one RKHS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5QnzWCDo_-uK"
   },
   "source": [
    "The following theorem proves the equivalence between a positive definite kernel and a reproducing kernel and will allow us to prove the fundamental theorem which says that any positive definite kernel can be represented as an inner product in some Hilbert space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPRBvqkeyM3X"
   },
   "source": [
    "**Theorem : A function $K : X^2 \\to R$ is a positive definite kernel if and only if it is a reproducing kernel for some Hilbert space H.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXq69b8-AZI_"
   },
   "source": [
    "> **Proof :**  \n",
    ">($\\Leftarrow$) If K is a reproducing kernel for a Hilbert space H, then it can be expressed as :\n",
    "$$K(x, y) = K_x(y) = \\langle K_x, K_y \\rangle$$\n",
    "Hence, K is symmetric by definition of the inner product in H and\n",
    "$$\\forall x_1, ... x_n \\in X^n, \\forall a_1, ... a_n \\in R, \n",
    "\\\\\\sum_i \\sum_j a_i a_j K(x_i, x_j) \n",
    "\\\\= \\langle \\sum_i a_i K_{x_i}, \\sum_j a_j K_{x_j} \\rangle \n",
    "\\\\= || \\sum_i a_i K_{x_i} || ^2 \\geq 0$$\n",
    "Then, K is a positive definite kernel.  \n",
    ">($\\Rightarrow$) K is a positive definite kernel. We need to create a RKHS H for which K will be the reproducing kernel.  \n",
    "Let $H_0$ be the vector subspace of $R^X$ spanned by the functions $(K_x)_{x \\in X}$ :\n",
    "$$H_0 = vect((K(x, .))_{x \\in X})$$   \n",
    "We want to define an inner product such that $H_0$ is an pre-Hilbert space.  \n",
    "For any $f,g \\in H_0^2$, given by $f = \\sum_{i=1}^m a_i K_{x_i}$, $g = \\sum_{j=1}^n b_j K_{x_j}$, let :\n",
    "$$\\langle f,g \\rangle _{H_0} = \\sum_i \\sum_j a_i b_j K(x_i, x_j)$$\n",
    "We can show that $(H_0, \\langle .,. \\rangle)$ is a pre-Hilbert space and K verifies the reproducing kernel conditions.  \n",
    "We can observe that any Cauchy sequences of $H_0$ converges to f, which is not necessarily in $H_0$. Thus, we don't have the completeness, so $H_0$ is not an Hilbert space. We then extends $H_0$ by creating $H \\subset R^X$ to be the set of functions $f : X \\to R$ which are pointwise limits of Cauchy sequences (of functions) of $H_0$.  \n",
    "We can observe that $H_0 \\subset H$ by taking the following sequences $(f_n = f)_{n\\in N} \\in H_0^N$ which converges pointwise to any $f \\in H_0$.  \n",
    "We define the following inner product in H :\n",
    "$$\\langle f,g \\rangle_H = lim_{n \\to \\infty} \\langle f_n, g_n \\rangle$$\n",
    "We can observe that this limit exists and is unique. What's more, it is easy to see that $\\langle .,. \\rangle_H$ is an inner product, using the same properties of $\\langle .,. \\rangle_{H_0}$.  \n",
    "By construction, we can show that H is complete and that K is a reproducing kernel for H (in particular, the reproducing property holds)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-K2uBnSkD_YR"
   },
   "source": [
    "Finally, we can deduce easily the Aronszajn's theorem (the general case of Mercer's theorem)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T5zL7nTKEOGS"
   },
   "source": [
    "**Theorem : Aronszajn's theorem  \n",
    "K is a positive definite kernel on the set X if and only if there exists a Hilbert space H and a mapping $\\phi : X \\to H$ such that, for any $x, y \\in X^2$ :\n",
    "$$ K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle _H $$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0B2EOjNFR3d"
   },
   "source": [
    "> **Proof :**  \n",
    "($\\Leftarrow$) Already proved.  \n",
    "($\\Rightarrow$) We proved that if K is a positive definite kernel then there exists a Hilbert space H such that K is a reproducing kernel for H. If we define the mapping $\\phi : X \\to H$ by :\n",
    "$$ \\forall x \\in X, \\phi(x) = K_x = K(x, .)$$\n",
    "Then, by reproducing property, we have :\n",
    "$$ \\forall (x, y) \\in X^2, \\langle \\phi(x), \\phi(y) \\rangle _H = \\langle K_x, K_y \\rangle _H = K_x(y) = K(x, y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t0w5VH-tGnJK"
   },
   "source": [
    "### 3) My first kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCD7DOa7Gx6I"
   },
   "source": [
    "Let's see some kernel examples and discover the RKHS associated to these kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8quFrTBVG4H0"
   },
   "source": [
    "### 4) Smoothness functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CW9UYgZAG7Ju"
   },
   "source": [
    "There is a natural way to regularize functions in a RKHS. Indeed, by Cauchy-Schwarz we have, for any $f \\in H$ and any two points $x,y \\in X$ :\n",
    "$$ |f(x) - f(y)| = | \\langle f, K_x - K_y \\rangle_H | \\leq || f ||_H . || K_x - K_y ||_H = || f ||_H . d_K (x, y) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The norm of a function in the RKHS controls **how fast** the function varies over X with respect to **the geometry defined by the kernel** (smooth with constant $‖f‖_H$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uxafe_xjHA1L"
   },
   "source": [
    "### 5) The kernel trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1mKHEnv5HDPu"
   },
   "source": [
    "We can show that kernel methods allow us to create efficient nonlinear methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mje2M3awHK4S"
   },
   "source": [
    "## II) Kernel methods : supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularized empirical risk formulation :**  \n",
    "The goal is to learn a **prediction function** $f : X \\to Y$ given labeled training data $(x_i \\in X, y_i \\in Y)_{1 \\leq i \\leq n}$ :\n",
    "$$min_{f \\in H} \\frac{1}{n} \\sum_{i=1}^n L(y_i, f(x_i)) + \\lambda ||f||_H^2$$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the new perspectives with kernel methods ?\n",
    "- being able to deal with non-linear functional spaces endowed with a natural regularization function $||.||^2_H$\n",
    "- being able to deal with non-vectorial data (graphs, tress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two theoretical results underpin a family of powerful algorithms for data analysis using positive definite kernels, collectively known as kernel methods:\n",
    "- The **kernel trick**, based on the representation of positive definite kernels as inner products\n",
    "- The **representer theorem**, based on some properties of the regularization functional defined by the RKHS norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ku8FPgFaHPSO"
   },
   "source": [
    "### 1) The representer theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cjxbh0jwHbN1"
   },
   "source": [
    "The representer theorem says that the solution to a regularized empirical risk minimization problem in a RKHS lives in the vector subspace spanned by the kernel functions, which is a concrete optimization problem in $R^n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem : the representer theorem  \n",
    "Let X be a set endowed with a positive definite kernel K, $H_K$ the corresponding RKHS, and $S = \\{x_1, ... x_n\\} \\subset X$ a finite set of points in X. Let $\\Psi : R^{n+1} \\to R$ be a function of n+1 variables, strictly increasing with respect to the last variable.  \n",
    "Then, any solution to the optimization problem :\n",
    "$$min_{f \\in H_K} \\Psi(f(x_1), ... f(x_n), ||f||_{H_K})$$\n",
    "admits a representation of the form :\n",
    "$$\\forall x \\in X, f(x) = \\sum_{i=1}^n \\alpha_i K(x_i, x)$$**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Proof :**\n",
    "Let V be the linear space spanned by $(K(x_i, .))_{1 \\leq i \\leq n}$. \n",
    "$$ V = \\{ f \\in H_K | f(x) = \\sum_{i=1}^n \\alpha_i K(x_i, x), (\\alpha_1, ... \\alpha_n) \\in R^n \\} $$\n",
    "V is a finite-dimensional subspace of H, therefore any function $f \\in H_K$ can be uniquely decomposed as : $f = f_V + f_{V^T}$ with $f_V \\in V$ and $f_{V^T} \\in V^T$. Since $H_K$ is a RKHS with kernel K, for $1 \\leq i \\leq n$,\n",
    "$f_{V^T} (x_i) = \\langle f, K_{x_i} \\rangle$ and $f(x_i) = \\langle f, K_{x_i} \\rangle = f_V (x_i)$.  \n",
    "Pythagora's theorem in $H_K$ then shows that : $||f||_{H_K}^2 = ||f_V||_{H_K}^2 + ||f_{V^T}||_{H_K}^2$.  \n",
    "Let $\\epsilon (f)$ be the function which is minimized in the statement of the representer theorem. As a consequence,\n",
    "$$\\epsilon (f) \\geq \\epsilon (f_V)$$\n",
    "which is equality if and only if $||f_{V^T}||=0$.  \n",
    "**The minimum of $\\Psi$ is therefore necessarily in V.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often the function $\\Psi$ has the form :\n",
    "$$ \\Psi(f(x_1), ... f(x_n), ||f||_{H_K}) = c(f(x_1), ... f(x_n)) + \\lambda \\Omega(||f||_{H_K})$$\n",
    "where $c(.)$ measures the \"fit\" of f to a given problem (regression, classification, dimension reduction, ...) and $\\Omega$ is strictly increasing.\n",
    "\n",
    "This formulation has two important consequences :\n",
    "- **Theoretically**, the minimization will enforce the norm $‖f‖_{H_K}$ to be \"small\", which can be beneficial by ensuring a sufficient level of smoothness for the solution (regularization effect).\n",
    "- **Practically**, we know by the representer theorem that the solution lives in a subspace of dimension n, which can lead to efficient algorithms although the RKHS itself can be of infinite dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most kernel methods have two complementary interpretations :\n",
    "- A **geometric interpretation** in the feature space, thanks to the kernel trick. Even when the feature space is “large”, most kernel methods work in the linear span of the embeddings of the points available.\n",
    "- A **functional interpretation**, often as an optimization problem over (subsets of) the RKHS associated to the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G8BFmhdNH4dg"
   },
   "source": [
    "### 2) Kernel ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wc5ellNEH6f6"
   },
   "source": [
    "Kernel ridge regression is a useful extension of ridge regression by searching a solution function in a RKHS. This extension is allowed thanks to the kernel trick. By the representer theorem, the solution can be easily written and compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_rwanda = pd.read_csv('./data/rwanda')\n",
    "X = df_rwanda['mean_light']\n",
    "y = df_rwanda['wealth_index']\n",
    "X = np.resize(X, (X.shape[0], 1))\n",
    "y = np.resize(y, (y.shape[0], 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "X_high, y_high, X_0, y_0 = [], [], [], []\n",
    "for i in range(X.shape[0]):\n",
    "    if X[i][0] < 5:\n",
    "        X_0.append(X[i])\n",
    "        y_0.append(y[i])\n",
    "    else:\n",
    "        X_high.append(X[i])\n",
    "        y_high.append(y[i])\n",
    "X_0 = np.resize(X_0, (len(X_0), 1))\n",
    "y_0 = np.resize(y_0, (len(y_0), 1))\n",
    "X_high = np.resize(X_high, (len(X_high), 1))\n",
    "y_high = np.resize(y_high, (len(y_high), 1))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_high, y_high, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regression(estimator, X_train, y_train, X_test, y_test):\n",
    "    x_axis = np.linspace(0, 64, 10000).reshape((10000, 1))\n",
    "    y_axis = estimator.predict(x_axis)\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.grid()\n",
    "    plt.xlim(-1, 64)\n",
    "    plt.ylim(-2, 5)\n",
    "    plt.title('Wealth prediction according to nightlight intensity')\n",
    "    plt.xlabel('nightlight intensity')\n",
    "    plt.ylabel('wealth')\n",
    "    plt.scatter(X_train, y_train)\n",
    "    plt.scatter(X_test, y_test, color='r')\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    plt.show()\n",
    "def plot_mse(class_estimator, alpha_min, alpha_max, precision, X_train, y_train, X_test, y_test):\n",
    "    alpha_list = np.linspace(alpha_min, alpha_max, precision)\n",
    "    train_MSE = []\n",
    "    test_MSE = []\n",
    "    for alpha in alpha_list:\n",
    "        estimator = class_estimator(int(alpha))\n",
    "        estimator.train(X_train, y_train)\n",
    "        train_MSE.append(estimator.train_MSE)\n",
    "        test_MSE.append(np.linalg.norm(y_test - estimator.predict(X_test)) / y_test.shape[0])\n",
    "    plt.plot(alpha_list, test_MSE, color='red')\n",
    "    plt.plot(alpha_list, train_MSE, color='green')\n",
    "    plt.xlabel('hyperparameter')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelRidgeRegression:\n",
    "    \n",
    "    def __init__(self, bandwidth, gamma=1e-5):\n",
    "        self.coeff = None\n",
    "        self.bandwidth = bandwidth\n",
    "        self.gamma = gamma\n",
    "        self.X = None\n",
    "        self.train_MSE = None\n",
    "    \n",
    "    def kernel(self, x, y):\n",
    "        return np.exp(- 0.5 * np.linalg.norm(x - y)**2 / self.bandwidth**2)\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X = X_train\n",
    "        K = np.array([[self.kernel(X_train[i], X_train[j]) for j in range(X_train.shape[0])] \n",
    "                       for i in range(X_train.shape[0])])\n",
    "        self.coeff = scipy.linalg.solve(K + self.gamma * np.identity(K.shape[0]), y_train)\n",
    "        self.train_MSE = np.sqrt(np.linalg.norm(y_train - self.predict(X_train)) ** 2 / y_train.shape[0])\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        K = np.array([[self.kernel(X_test[i], X_train[j]) for j in range(X_train.shape[0])] \n",
    "                      for i in range(X_test.shape[0])])\n",
    "        y_test = np.dot(K, self.coeff)\n",
    "        return y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = KernelRidgeRegression(bandwidth=10., gamma=1e-5)\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "plot_regression(estimator, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print('MSE on train data :', estimator.train_MSE)\n",
    "print('MSE on test data :', np.sqrt(np.linalg.norm(y_test - estimator.predict(X_test)) ** 2 / y_test.shape[0]))\n",
    "\n",
    "#plot_mse(KernelRidgeRegression, 1, 100, 100, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jgUs-Ej5JTEg"
   },
   "source": [
    "Bibliographie :\n",
    "\n",
    "http://lear.inrialpes.fr/people/mairal/teaching/2015-2016/MVA/fichiers/mva_slides.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wK4jbGs887Ly"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "kernel_methods.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
